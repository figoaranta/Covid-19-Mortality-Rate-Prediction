{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "## 1. Data Cleaning\n",
    "## 2. Data Exploration\n",
    "## 3. Feature Engineering\n",
    "## 4. Data Preprocessing\n",
    "## 5. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('owid-covid-data.csv')\n",
    "data.dropna(subset=['iso_code'],inplace=True)\n",
    "pd.set_option(\"max_rows\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = data[['iso_code','continent','location',\n",
    "                'population','human_development_index','gdp_per_capita',\n",
    "                'date','total_cases','total_deaths',\n",
    "                'new_cases','new_deaths','new_cases_smoothed',\n",
    "                'new_deaths_smoothed','total_cases_per_million','total_deaths_per_million',\n",
    "                'new_cases_per_million','new_deaths_per_million','new_cases_smoothed_per_million',\n",
    "                'new_deaths_smoothed_per_million','diabetes_prevalence','cardiovasc_death_rate',\n",
    "                'median_age','handwashing_facilities','hospital_beds_per_thousand',\n",
    "                'life_expectancy','stringency_index','total_tests',\n",
    "                'new_tests','total_tests_per_thousand','new_tests_per_thousand',\n",
    "                'new_tests_smoothed','new_tests_smoothed_per_thousand','tests_per_case',\n",
    "                'extreme_poverty']]\n",
    "\n",
    "newData['id'] = range(1,len(newData)+1)\n",
    "cols = newData.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "newData = newData[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spitting data into testing and training.\n",
    "training = pd.DataFrame()\n",
    "testing = pd.DataFrame()\n",
    "\n",
    "# Split 20% for each location in the data.\n",
    "for i in newData['location'].value_counts().index:\n",
    "    train, test = train_test_split(newData[newData['location']==i], test_size=0.2, shuffle=False)\n",
    "    training = pd.concat([training,train])\n",
    "    testing = pd.concat([testing,test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['train_data'] = 1\n",
    "testing['train_data'] = 0\n",
    "newData = pd.concat([training,testing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.describe().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = training[['iso_code','continent','location','date']]\n",
    "df_num = training[['population', 'human_development_index', 'gdp_per_capita',\n",
    "       'total_cases', 'total_deaths', 'new_cases', 'new_deaths',\n",
    "       'new_cases_smoothed', 'new_deaths_smoothed', 'total_cases_per_million',\n",
    "       'total_deaths_per_million', 'new_cases_per_million',\n",
    "       'new_deaths_per_million', 'new_cases_smoothed_per_million',\n",
    "       'new_deaths_smoothed_per_million', 'diabetes_prevalence',\n",
    "       'cardiovasc_death_rate', 'median_age', 'handwashing_facilities',\n",
    "       'hospital_beds_per_thousand', 'life_expectancy', 'stringency_index',\n",
    "       'total_tests', 'new_tests', 'total_tests_per_thousand',\n",
    "       'new_tests_per_thousand', 'new_tests_smoothed',\n",
    "       'new_tests_smoothed_per_thousand', 'tests_per_case', 'extreme_poverty']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red','orange','blue','green','yellow','purple','darkblue','pink','lightblue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_num:\n",
    "    plt.hist(df_num[i],range=(df_num[i].min(),df_num[i].max()),color=colors[random.randrange(0,len(colors)-1)])\n",
    "    plt.title(\"Histogram of \"+i)\n",
    "    plt.ylabel('Frequency of Occurrence')\n",
    "    plt.xlabel(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_num:\n",
    "    df_num[i].value_counts().plot(kind='bar')\n",
    "    plt.title(\"Bar Chart of \"+i)\n",
    "    plt.ylabel('Frequency of Occurrence')\n",
    "    plt.xlabel(i)\n",
    "    sns.despine\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_num.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_num.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_cat.columns:\n",
    "    sns.barplot(df_cat[i].value_counts().index , df_cat[i].value_counts()).set_title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in newData['location'].value_counts().index:\n",
    "    y_axis = newData[newData['location']== i]['new_deaths_smoothed']\n",
    "    x_axis = newData[newData['location']== i]['date']\n",
    "    plt.plot(x_axis,y_axis,color=colors[random.randrange(0,len(colors)-1)])\n",
    "    plt.title(\"Graph of the number of death in \"+i)\n",
    "    plt.ylabel('New Deaths')\n",
    "    plt.xlabel('Date')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in newData['continent'].value_counts().index:\n",
    "    for j in newData[newData['continent']==i]['location'].value_counts().index:\n",
    "        x_axis = newData[newData['location']== j]['date']\n",
    "        plt.plot(x_axis,newData[newData['location']== j]['new_deaths_smoothed'],label=j)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of New Death')\n",
    "    plt.legend(framealpha=1,bbox_to_anchor=(1.05, 1),loc='upper left');\n",
    "    plt.title('New death in '+i+' continent')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in newData['location'].value_counts().index:\n",
    "    y_axis = newData[newData['location']== i]['new_deaths_smoothed']\n",
    "    x_axis = newData[newData['location']== i]['stringency_index']\n",
    "    plt.scatter(x_axis,y_axis,color=colors[random.randrange(0,len(colors)-1)])\n",
    "    plt.title(\"Correlation of \"+ \"the number of death and the Stringency Index in \"+i)\n",
    "    plt.ylabel('New Deaths')\n",
    "    plt.xlabel('Stringency Index')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in newData['continent'].value_counts().index:\n",
    "    population = newData[newData['continent'] == i]['population'].value_counts().index\n",
    "    tick_label = newData[newData['continent'] == i]['location'].value_counts().index\n",
    "    x_coordinate = np.arange(1,len(tick_label)+1)\n",
    "    ax = sns.barplot(population, tick_label)\n",
    "    ax.set_title(\"Barplot of population in \"+ j +\" continent\")\n",
    "    ax.set_xlabel(\"Population\")\n",
    "    ax.set_ylabel(\"Location\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in newData['location'].value_counts().index:\n",
    "    y_axis = newData[newData['location']== i]['new_deaths_smoothed']\n",
    "    x_axis = newData[newData['location']== i]['tests_per_case']\n",
    "    plt.scatter(x_axis,y_axis,color=colors[random.randrange(0,len(colors)-1)])\n",
    "    plt.title(\"Correlation of \"+ \"the number of death and the number of Test per case \"+i)\n",
    "    plt.ylabel('New Deaths')\n",
    "    plt.xlabel('Test per case')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(training, index = 'continent', columns = 'location', values = 'id' ,aggfunc ='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(training, index = 'continent', columns = 'new_deaths', values = 'id' ,aggfunc ='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extremePoverty_newDeaths_corr = newData[['extreme_poverty','new_deaths_smoothed']]\n",
    "extremePoverty_newDeaths_corr.dropna(subset=['extreme_poverty'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=extremePoverty_newDeaths_corr ,x = 'extreme_poverty' , y = 'new_deaths_smoothed',hue=\"extreme_poverty\",marker=\"+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(training, index = 'continent', values = ['population','new_deaths','handwashing_facilities','stringency_index','extreme_poverty','handwashing_facilities','tests_per_case','gdp_per_capita'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData['date_to_integer'] = newData['date'].apply(lambda x : float(''.join(x.split('-'))) if str(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData['continent'] = newData['continent'].apply(lambda x : \"World\" if pd.isna(x) else x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdi_continent_mean = dict()\n",
    "hdi_location_mean = dict()\n",
    "gdp_location_mean = dict()\n",
    "\n",
    "for i in (newData['continent'].value_counts().index):\n",
    "    hdi_continent_mean[i] = newData[newData['continent'] == i]['human_development_index'].mean()\n",
    "\n",
    "for i in newData['location'].value_counts().index:\n",
    "    hdi_location_mean[i] = newData[newData['location'] == i]['human_development_index'].mean()\n",
    "\n",
    "for key,value in hdi_location_mean.items():\n",
    "    if pd.isna(value):\n",
    "        hdi_location_mean[key]= hdi_continent_mean[newData[newData['location']==key]['continent'].values[0]] \n",
    "        \n",
    "for i in newData['location'].value_counts().index:\n",
    "    if pd.isna(newData[newData['location']==i]['gdp_per_capita'].values[0]):\n",
    "        gdp_location_mean[i] = newData[newData['continent'] == newData[newData['location']==i]['continent'].values[0]]['gdp_per_capita'].mean()\n",
    "    else:\n",
    "        gdp_location_mean[i] = newData[newData['location']==i]['gdp_per_capita'].mean()\n",
    "        \n",
    "hdi_location_mean.pop('World')\n",
    "hdi_continent_mean.pop('World')\n",
    "hdi_location_mean['World'] = sum(hdi_location_mean.values())/len(hdi_location_mean)\n",
    "hdi_continent_mean['World'] = sum(hdi_continent_mean.values())/len(hdi_continent_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData['human_development_index'] = newData['location'].apply(lambda x:  hdi_location_mean[x])  \n",
    "newData['gdp_per_capita'] = newData['location'].apply(lambda x:  gdp_location_mean[x])\n",
    "newData['total_cases'] = newData['total_cases'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "newData['total_deaths'] = newData['total_deaths'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "newData['new_deaths'] = newData['new_deaths'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "newData['new_cases'] = newData['new_cases'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "newData['new_cases_smoothed'] = newData['new_cases_smoothed'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "newData['new_deaths_smoothed'] = newData['new_deaths_smoothed'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "newData['total_cases_per_million'] = newData['total_cases_per_million'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "newData['total_deaths_per_million'] = newData['total_deaths_per_million'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "newData['new_cases_per_million'] = newData['new_cases_per_million'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "newData['new_deaths_per_million'] = newData['new_deaths_per_million'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "newData['new_cases_smoothed_per_million'] = newData['new_cases_smoothed_per_million'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "newData['new_deaths_smoothed_per_million'] = newData['new_deaths_smoothed_per_million'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "newData['total_tests'] = newData['total_tests'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "newData['new_tests'] = newData['new_tests'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "newData['total_tests_per_thousand'] = newData['total_tests_per_thousand'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "newData['new_tests_per_thousand'] = newData['new_tests_per_thousand'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "newData['new_tests_smoothed'] = newData['new_tests_smoothed'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "newData['new_tests_smoothed_per_thousand'] = newData['new_tests_smoothed_per_thousand'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "newData['tests_per_case'] = newData['tests_per_case'].apply(lambda x: 0 if pd.isna(x) else x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_prevalence_location = dict()\n",
    "cardiovasc_death_rate_location = dict()\n",
    "median_age_location = dict()\n",
    "handwashing_facilities_location = dict()\n",
    "hospital_beds_per_thousand_location = dict()\n",
    "life_expectancy_location = dict()\n",
    "stringency_index_location = dict()\n",
    "extreme_poverty_location = dict()\n",
    "\n",
    "for i in newData['location'].value_counts().index:\n",
    "    diabetes_prevalence_location[i] = newData[newData['location']==i]['diabetes_prevalence'].values[0]\n",
    "    cardiovasc_death_rate_location[i] = newData[newData['location']==i]['cardiovasc_death_rate'].values[0]\n",
    "    median_age_location[i] = newData[newData['location']==i]['median_age'].values[0]\n",
    "    handwashing_facilities_location[i] = newData[newData['location']==i]['handwashing_facilities'].values[0]\n",
    "    hospital_beds_per_thousand_location[i] = newData[newData['location']==i]['hospital_beds_per_thousand'].values[0]\n",
    "    life_expectancy_location[i] = newData[newData['location']==i]['life_expectancy'].values[0]\n",
    "    stringency_index_location[i] = newData[newData['location']==i]['stringency_index'].values[0]\n",
    "    extreme_poverty_location[i] = newData[newData['location']==i]['extreme_poverty'].values[0]\n",
    "    \n",
    "    if pd.isna(newData[newData['location']==i]['diabetes_prevalence'].values[0]):\n",
    "        diabetes_prevalence_location[i] = newData[newData['continent'] == newData[newData['location']==i]['continent'].values[0]]['diabetes_prevalence'].median()\n",
    "    if pd.isna(newData[newData['location']==i]['cardiovasc_death_rate'].values[0]):\n",
    "        cardiovasc_death_rate_location[i] = newData[newData['continent'] == newData[newData['location']==i]['continent'].values[0]]['cardiovasc_death_rate'].median()\n",
    "    if pd.isna(newData[newData['location']==i]['median_age'].values[0]):\n",
    "        median_age_location[i] = newData[newData['continent'] == newData[newData['location']==i]['continent'].values[0]]['median_age'].median() \n",
    "    if pd.isna(newData[newData['location']==i]['handwashing_facilities'].values[0]):\n",
    "        handwashing_facilities_location[i] = newData[newData['continent'] == newData[newData['location']==i]['continent'].values[0]]['handwashing_facilities'].mean() \n",
    "    if pd.isna(newData[newData['location']==i]['hospital_beds_per_thousand'].values[0]):\n",
    "        hospital_beds_per_thousand_location[i] = newData[newData['continent'] == newData[newData['location']==i]['continent'].values[0]]['hospital_beds_per_thousand'].mean()\n",
    "    if pd.isna(newData[newData['location']==i]['life_expectancy'].values[0]):\n",
    "        life_expectancy_location[i] = newData[newData['continent'] == newData[newData['location']==i]['continent'].values[0]]['life_expectancy'].median()\n",
    "    if pd.isna(newData[newData['location']==i]['stringency_index'].values[0]):\n",
    "        stringency_index_location[i] = newData[newData['continent'] == newData[newData['location']==i]['continent'].values[0]]['stringency_index'].mean()\n",
    "    if pd.isna(newData[newData['location']==i]['extreme_poverty'].values[0]):\n",
    "        extreme_poverty_location[i] = newData[newData['continent'] == newData[newData['location']==i]['continent'].values[0]]['extreme_poverty'].mean()\n",
    "\n",
    "stringency_index_location.pop('World')\n",
    "stringency_index_location['World'] = sum(stringency_index_location.values())/len(stringency_index_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData['diabetes_prevalence'] = newData['location'].apply(lambda x:  diabetes_prevalence_location[x])  \n",
    "newData['cardiovasc_death_rate'] = newData['location'].apply(lambda x:  cardiovasc_death_rate_location[x])  \n",
    "newData['median_age'] = newData['location'].apply(lambda x:  median_age_location[x])  \n",
    "newData['handwashing_facilities'] = newData['location'].apply(lambda x:  handwashing_facilities_location[x])  \n",
    "newData['hospital_beds_per_thousand'] = newData['location'].apply(lambda x:  hospital_beds_per_thousand_location[x])  \n",
    "newData['life_expectancy'] = newData['location'].apply(lambda x:  life_expectancy_location[x])  \n",
    "newData['stringency_index'] = newData['location'].apply(lambda x:  stringency_index_location[x])  \n",
    "newData['extreme_poverty'] = newData['location'].apply(lambda x:  extreme_poverty_location[x])\n",
    "newData['stringency_index'] = newData['location'].apply(lambda x:  stringency_index_location[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData.drop(newData[newData['new_deaths_smoothed'] < 0].index, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dummies = pd.get_dummies(newData[['new_deaths_smoothed','population','human_development_index','gdp_per_capita','date_to_integer','new_cases_smoothed','diabetes_prevalence','cardiovasc_death_rate','median_age','handwashing_facilities','life_expectancy','stringency_index','new_tests_smoothed','new_tests_smoothed_per_thousand','tests_per_case','extreme_poverty','train_data']])\n",
    "X_train = all_dummies[all_dummies['train_data']==1].drop(['train_data'],axis=1)\n",
    "X_test = all_dummies[all_dummies['train_data']==0].drop(['train_data'],axis=1)\n",
    "\n",
    "Y_train = newData[newData['train_data']==1]['new_deaths_smoothed']\n",
    "Y_test = newData[newData['train_data']==0]['new_deaths_smoothed']\n",
    "Y_train = Y_train.astype('int')\n",
    "Y_test = Y_test.astype('int')\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scale the data for our model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler()\n",
    "all_dummies_scaled = all_dummies.copy()\n",
    "all_dummies_scaled[['new_deaths_smoothed', 'population', 'human_development_index',\n",
    "       'gdp_per_capita', 'date_to_integer', 'new_cases_smoothed',\n",
    "       'diabetes_prevalence', 'cardiovasc_death_rate', 'median_age',\n",
    "       'handwashing_facilities', 'life_expectancy', 'stringency_index',\n",
    "       'new_tests_smoothed', 'new_tests_smoothed_per_thousand',\n",
    "       'tests_per_case', 'extreme_poverty']]= scale.fit_transform(all_dummies_scaled[['new_deaths_smoothed', 'population', 'human_development_index',\n",
    "       'gdp_per_capita', 'date_to_integer', 'new_cases_smoothed',\n",
    "       'diabetes_prevalence', 'cardiovasc_death_rate', 'median_age',\n",
    "       'handwashing_facilities', 'life_expectancy', 'stringency_index',\n",
    "       'new_tests_smoothed', 'new_tests_smoothed_per_thousand',\n",
    "       'tests_per_case', 'extreme_poverty']])\n",
    "\n",
    "X_train_scaled = all_dummies_scaled[all_dummies_scaled['train_data'] == 1].drop(['train_data'],axis=1).drop(['new_deaths_smoothed'],axis=1)\n",
    "X_test_scaled = all_dummies_scaled[all_dummies_scaled['train_data'] == 0].drop(['train_data'],axis=1).drop(['new_deaths_smoothed'],axis=1)\n",
    "\n",
    "Y_train_scaled = all_dummies_scaled[all_dummies_scaled['train_data']==1]['new_deaths_smoothed']\n",
    "Y_test_scaled = all_dummies_scaled[all_dummies_scaled['train_data']==0]['new_deaths_smoothed']\n",
    "Y_train_scaled = Y_train_scaled.astype('int')\n",
    "Y_test_scaled = Y_test_scaled.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dummies_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_scaled.value_counts().index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "cv = cross_val_score(gnb,X_train_scaled,Y_train,cv=5)\n",
    "print(cv)\n",
    "print(cv.mean())\n",
    "# gnb.fit(X_train_scaled,Y_train)\n",
    "# gnb.score(X_test_scaled,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()#max_iter=100000\n",
    "# cv = cross_val_score(lr,X_train_scaled,Y_train,cv=5)\n",
    "# print(cv)\n",
    "# print(cv.mean())\n",
    "lr.fit(X_train_scaled,Y_train)\n",
    "lr.score(X_test_scaled,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "# cv = cross_val_score(lin_reg,X_train_scaled,Y_train,cv=5) #,scoring=\"neg_mean_absolute_error\"\n",
    "# print(cv)\n",
    "# print(cv.mean())\n",
    "lin_reg.fit(X_train_scaled,Y_train)\n",
    "lin_reg.score(X_test_scaled,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(n_estimators = 400, max_depth = 5, min_samples_split = 2,\n",
    "          learning_rate = 0.1, loss = 'ls')\n",
    "# cv = cross_val_score(lin_reg,X_train_scaled,Y_train,cv=5) #,scoring=\"neg_mean_absolute_error\"\n",
    "# print(cv)\n",
    "# print(cv.mean())\n",
    "gbr.fit(X_train_scaled,Y_train)\n",
    "gbr.score(X_test_scaled,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rr = Ridge() \n",
    "# cv = cross_val_score(rr,X_train_scaled,Y_train,cv=5)\n",
    "# print(cv)\n",
    "# print(cv.mean())\n",
    "rr.fit(X_train_scaled,Y_train)\n",
    "rr.score(X_test_scaled,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(probability = True)\n",
    "# cv = cross_val_score(svc,X_train_scaled,Y_train,cv=5)\n",
    "# print(cv)\n",
    "# print(cv.mean())\n",
    "svc.fit(X_train_scaled,Y_train)\n",
    "svc.score(X_test_scaled,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier() #random_state = 1\n",
    "# cv = cross_val_score(rf,X_train_scaled,Y_train,cv=5)\n",
    "# print(cv)\n",
    "# print(cv.mean())\n",
    "rf.fit(X_train_scaled,Y_train)\n",
    "rf.score(X_test_scaled,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = VotingClassifier(estimators = [('lr',lr),('lin_reg',lin_reg),('rr',rr),('gbr',gbr),('rf',rf)], voting = 'soft')\n",
    "# cv = cross_val_score(voting,X_train_scaled,Y_train,cv=5)\n",
    "# print(cv)\n",
    "# print(cv.mean())\n",
    "voting.fit(X_train_scaled,Y_train)\n",
    "voting.score(X_test_scaled,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import RandomizedSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_performance(classifier, model_name):\n",
    "    print(model_name)\n",
    "    print('Best Score: ' + str(classifier.best_score_))\n",
    "    print('Best Parameters: ' + str(classifier.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "param_grid = {'max_iter' : [2000],\n",
    "              'dual':[False],\n",
    "              'penalty' : ['l2'],\n",
    "              'C' : np.logspace(-4, 4, 20),\n",
    "              'fit_intercept':[True,False],\n",
    "              'intercept_scaling':np.logspace(-4, 4, 20),\n",
    "              'max_iter':[1000000],\n",
    "              'solver' : ['liblinear','lbfgs','newton-cg','sag','saga'],\n",
    "              'multi_class' : ['auto','ovr','multinomial']\n",
    "    \n",
    "}\n",
    "clf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5 , verbose = True , n_jobs = -1)\n",
    "best_clf_lr = clf_rf.fit(X_train_scaled,Y_train)\n",
    "clf_performance(best_clf_lr,'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "param_grid = {'fit_intercept':[True,False],\n",
    "              'normalize':[True,False],\n",
    "              'copy_X':[True,False],\n",
    "}\n",
    "clf_lin_reg = GridSearchCV(lin_reg, param_grid = param_grid, cv = 5 , verbose = True , n_jobs = -1)\n",
    "best_clf_lin_reg = clf_lin_reg.fit(X_train_scaled,Y_train)\n",
    "clf_performance(best_clf_lin_reg,'Linear Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor()\n",
    "param_grid = {'n_estimators' : [350,400,450,500],\n",
    "              'max_depth':[5,10,15,20],\n",
    "              'min_samples_split' :[2,5,7,10],\n",
    "              'learning_rate' : [0.1,0.5,1,2],\n",
    "              'loss' : ['ls','lad','huber','quantile']\n",
    "}\n",
    "clf_gbr = GridSearchCV(gbr, param_grid = param_grid, cv = 5 , verbose = True , n_jobs = -1)\n",
    "best_clf_gbr  = clf_gbr.fit(X_train_scaled,Y_train)\n",
    "clf_performance(best_clf_gbr,'Gradient Boost Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = Ridge()\n",
    "param_grid = {'alpha' : [1.0,2.0,5.0,10.0],\n",
    "              'fit_intercept':[True,False],\n",
    "              'normalize' :[True,False],\n",
    "              'copy_X' : [True,False],\n",
    "              'max_iter' : [10000],\n",
    "              'solver':['auto','svd','cholesky','lsqr','sparse_cg']\n",
    "}\n",
    "clf_rr = GridSearchCV(rr, param_grid = param_grid, cv = 5 , verbose = True , n_jobs = -1)\n",
    "best_clf_rr  = clf_rr.fit(X_train_scaled,Y_train)\n",
    "clf_performance(best_clf_rr,'Ridge Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(probability = True)\n",
    "param_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n",
    "                                  'C': [.1, 1, 10, 100, 1000]},\n",
    "                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n",
    "                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\n",
    "clf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n",
    "best_clf_svc = clf_svc.fit(X_train_scaled,Y_train)\n",
    "clf_performance(best_clf_svc,'SVC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Randomized Search to narrow down the hyperparameter\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "clf_random_rf = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=True, random_state=42, n_jobs = -1)\n",
    "best_clf_random_rf.fit(X_train_scaled, Y_train)\n",
    "clf_performance(best_clf_random_rf,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use granular search from the results from randomized search\n",
    "rf = RandomForestClassifier(random_state = 1)\n",
    "param_grid =  {'n_estimators': [400,450,500,550],\n",
    "               'criterion':['gini','entropy'],\n",
    "                                  'bootstrap': [True],\n",
    "                                  'max_depth': [15, 20, 25],\n",
    "                                  'max_features': ['auto','sqrt', 10],\n",
    "                                  'min_samples_leaf': [2,3],\n",
    "                                  'min_samples_split': [2,3]}\n",
    "                                  \n",
    "clf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n",
    "best_clf_rf = clf_rf.fit(X_train_scaled,Y_train)\n",
    "clf_performance(best_clf_rf,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned Models\n",
    "tuned_lr = best_clf_lr.best_estimator_\n",
    "tuned_svc = best_clf_svc.best_estimator_\n",
    "tuned_rf = best_clf_rf.best_estimator_\n",
    "tuned_rr = best_clf_rr.best_estimator_\n",
    "tuned_lin_reg = best_clf_lin_reg.best_estimator_\n",
    "tuned_gbr = best_clf_gbr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = VotingClassifier(estimators = [('lr',best_lr),('rf',best_rf),('rr',tuned_rr),('lin_reg',tuned_lin_reg),('gbr',tuned_gbr)], voting = 'soft')\n",
    "cv = cross_val_score(voting,X_train_scaled,Y_train,cv=5)\n",
    "print(cv)\n",
    "print(cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the weights for each models in the voting classifier\n",
    "params = {'weights' : [[1,1,1,1,1],[1,2,1,2,1],[1,1,2,1,1],[2,1,1,1,2],[2,2,1,2,2],[1,2,2,2,1],[2,1,2,1,2]]}\n",
    "clf_voting = GridSearchCV(voting_clf, param_grid = params, cv = 5, verbose = True, n_jobs = -1)\n",
    "best_clf_voting = clf_voting.fit(X_train_scaled,Y_train)\n",
    "clf_performance(best_clf_voting,'Voting Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_voting = best_clf_voting.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tuned Logsitic Regression:',best_clf_lr.best_score_)\n",
    "print('Tuned Support Vector Machine:',best_clf_svc.best_score_)\n",
    "print('Tuned Random Forest:',best_clf_rf.best_score_)\n",
    "print('Tuned Voting Classifier:',best_clf_voting.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting & Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_lr.fit(X_train_scaled,Y_train)\n",
    "tuned_svc.fit(X_train_scaled,Y_train)\n",
    "tuned_rf.fit(X_train_scaled,Y_train)\n",
    "tuned_voting.fit(X_train_scaled,Y_train)\n",
    "\n",
    "pickle.dump(tuned_lr, open('lr_model', 'wb'))\n",
    "pickle.dump(tuned_svc, open('svc_model', 'wb'))\n",
    "pickle.dump(tuned_rf, open('rf_model', 'wb'))\n",
    "pickle.dump(tuned_voting, open('voting_model', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading & Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_lr = pickle.load(open('lr_model', 'rb'))\n",
    "loaded_svc = pickle.load(open('svc_model', 'rb'))\n",
    "loaded_rf = pickle.load(open('rf_model', 'rb'))\n",
    "loaded_voting = pickle.load(open('voting_model', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prediction = loaded_lr.predict(X_test_scaled).astype(int)\n",
    "svc_prediction = loaded_svc.predict(X_test_scaled).astype(int)\n",
    "rf_prediction = loaded_rf.predict(X_test_scaled).astype(int)\n",
    "voting_prediction = loaded_voting.predict(X_test_scaled).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_comparison = {'ID':testing['id'],'lr':lr_prediction,'svc':svc_prediction,'rf':rf_prediction,'voting':voting_prediction,'actual':Y_train}\n",
    "result_comparison = pd.DataFrame(data=result_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_comparison['difference_lr_svc'] = result_comparison.apply(lambda x : 1 if x['lr'] != x['svc'] else 0, axis = 1)\n",
    "result_comparison['difference_svc_rf'] = result_comparison.apply(lambda x : 1 if x['svc'] != x['rf'] else 0, axis = 1)\n",
    "result_comparison['differences_rf_voting'] = result_comparison.apply(lambda x : 1 if x['rf'] != x['voting'] else 0, axis = 1)\n",
    "result_comparison['differences_voting_lr'] = result_comparison.apply(lambda x : 1 if x['voting'] != x['lr'] else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_lr_svc = result_comparison['difference_lr_svc'].value_counts()\n",
    "diff_svc_rf = result_comparison['difference_svc_rf'].value_counts()\n",
    "diff_rf_voting = result_comparison['differences_rf_voting'].value_counts()\n",
    "diff_voting_lr = result_comparison['differences_voting_lr'].value_counts()\n",
    "\n",
    "print('Value difference between Tuned Logistic Regression and Tuned Support Vector Classifier :',diff_lr_svc.values[1])\n",
    "print('Value difference between Tuned Support Vector Classifier and Tuned Random Forest :',diff_svc_rf.values[1])\n",
    "print('Value difference between Tuned Random Forest and Tuned Voting Classifier:',diff_rf_voting.values[1])\n",
    "print('Value difference between Tuned Voting Classifier and Tuned Logistic Regression:',diff_voting_lr.values[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
